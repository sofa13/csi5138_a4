{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference GAN https://github.com/daymos/simple_keras_GAN/blob/master/gan.py\n",
    "#reference WGAN https://github.com/eriklindernoren/Keras-GAN/blob/master/wgan/wgan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class WGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "        self.g_loss = []\n",
    "        self.d_loss = []\n",
    "\n",
    "        # Following parameter and optimizer set as recommended in paper\n",
    "        self.n_critic = 5\n",
    "        self.clip_value = 0.01\n",
    "        optimizer = RMSprop(lr=0.00005)\n",
    "\n",
    "        # Build and compile the critic\n",
    "        self.critic = self.build_critic()\n",
    "        self.critic.compile(loss=self.wasserstein_loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.critic.trainable = False\n",
    "\n",
    "        # The critic takes generated images as input and determines validity\n",
    "        valid = self.critic(img)\n",
    "\n",
    "        # The combined model  (stacked generator and critic)\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss=self.wasserstein_loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=4, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_critic(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (X_test, _) = mnist.load_data()\n",
    "        X_train = X_test\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = -np.ones((batch_size, 1))\n",
    "        fake = np.ones((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for _ in range(self.n_critic):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Select a random batch of images\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                \n",
    "                # Sample noise as generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "                # Generate a batch of new images\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "                # Train the critic\n",
    "                d_loss_real = self.critic.train_on_batch(imgs, valid)\n",
    "                d_loss_fake = self.critic.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "\n",
    "                # Clip critic weights\n",
    "                for l in self.critic.layers:\n",
    "                    weights = l.get_weights()\n",
    "                    weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n",
    "                    l.set_weights(weights)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, 1 - d_loss[0], 1 - g_loss[0]))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "                \n",
    "            self.g_loss.append(g_loss)\n",
    "            self.d_loss.append(d_loss)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 1\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 14, 14, 16)        160       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 7, 7, 32)          4640      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 100,097\n",
      "Trainable params: 99,649\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 28, 28, 64)        131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 28, 28, 1)         1025      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,028,673\n",
      "Trainable params: 1,028,289\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "wgan = WGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sophie\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.999924] [G loss: 1.000163]\n",
      "1 [D loss: 0.999928] [G loss: 1.000163]\n",
      "2 [D loss: 0.999935] [G loss: 1.000170]\n",
      "3 [D loss: 0.999932] [G loss: 1.000166]\n",
      "4 [D loss: 0.999935] [G loss: 1.000166]\n",
      "5 [D loss: 0.999934] [G loss: 1.000167]\n",
      "6 [D loss: 0.999938] [G loss: 1.000171]\n",
      "7 [D loss: 0.999938] [G loss: 1.000165]\n",
      "8 [D loss: 0.999931] [G loss: 1.000177]\n",
      "9 [D loss: 0.999931] [G loss: 1.000162]\n",
      "10 [D loss: 0.999944] [G loss: 1.000167]\n",
      "11 [D loss: 0.999939] [G loss: 1.000161]\n",
      "12 [D loss: 0.999940] [G loss: 1.000157]\n",
      "13 [D loss: 0.999935] [G loss: 1.000134]\n",
      "14 [D loss: 0.999946] [G loss: 1.000139]\n",
      "15 [D loss: 0.999937] [G loss: 1.000134]\n",
      "16 [D loss: 0.999941] [G loss: 1.000135]\n",
      "17 [D loss: 0.999946] [G loss: 1.000111]\n",
      "18 [D loss: 0.999946] [G loss: 1.000111]\n",
      "19 [D loss: 0.999938] [G loss: 1.000102]\n",
      "20 [D loss: 0.999946] [G loss: 1.000093]\n",
      "21 [D loss: 0.999952] [G loss: 1.000081]\n",
      "22 [D loss: 0.999948] [G loss: 1.000099]\n",
      "23 [D loss: 0.999959] [G loss: 1.000093]\n",
      "24 [D loss: 0.999955] [G loss: 1.000099]\n",
      "25 [D loss: 0.999965] [G loss: 1.000081]\n",
      "26 [D loss: 0.999960] [G loss: 1.000098]\n",
      "27 [D loss: 0.999955] [G loss: 1.000092]\n",
      "28 [D loss: 0.999965] [G loss: 1.000079]\n",
      "29 [D loss: 0.999962] [G loss: 1.000094]\n",
      "30 [D loss: 0.999962] [G loss: 1.000084]\n",
      "31 [D loss: 0.999950] [G loss: 1.000096]\n",
      "32 [D loss: 0.999962] [G loss: 1.000084]\n",
      "33 [D loss: 0.999963] [G loss: 1.000089]\n",
      "34 [D loss: 0.999966] [G loss: 1.000083]\n",
      "35 [D loss: 0.999971] [G loss: 1.000091]\n",
      "36 [D loss: 0.999957] [G loss: 1.000071]\n",
      "37 [D loss: 0.999964] [G loss: 1.000099]\n",
      "38 [D loss: 0.999963] [G loss: 1.000077]\n",
      "39 [D loss: 0.999970] [G loss: 1.000068]\n",
      "40 [D loss: 0.999964] [G loss: 1.000072]\n",
      "41 [D loss: 0.999969] [G loss: 1.000064]\n",
      "42 [D loss: 0.999966] [G loss: 1.000072]\n",
      "43 [D loss: 0.999979] [G loss: 1.000069]\n",
      "44 [D loss: 0.999969] [G loss: 1.000072]\n",
      "45 [D loss: 0.999965] [G loss: 1.000061]\n",
      "46 [D loss: 0.999970] [G loss: 1.000064]\n",
      "47 [D loss: 0.999967] [G loss: 1.000071]\n",
      "48 [D loss: 0.999966] [G loss: 1.000063]\n",
      "49 [D loss: 0.999970] [G loss: 1.000078]\n",
      "50 [D loss: 0.999980] [G loss: 1.000057]\n",
      "51 [D loss: 0.999970] [G loss: 1.000060]\n",
      "52 [D loss: 0.999975] [G loss: 1.000058]\n",
      "53 [D loss: 0.999975] [G loss: 1.000061]\n",
      "54 [D loss: 0.999974] [G loss: 1.000062]\n",
      "55 [D loss: 0.999986] [G loss: 1.000069]\n",
      "56 [D loss: 0.999977] [G loss: 1.000064]\n",
      "57 [D loss: 0.999980] [G loss: 1.000066]\n",
      "58 [D loss: 0.999982] [G loss: 1.000060]\n",
      "59 [D loss: 0.999970] [G loss: 1.000073]\n",
      "60 [D loss: 0.999972] [G loss: 1.000063]\n",
      "61 [D loss: 0.999986] [G loss: 1.000080]\n",
      "62 [D loss: 0.999978] [G loss: 1.000065]\n",
      "63 [D loss: 0.999977] [G loss: 1.000078]\n",
      "64 [D loss: 0.999987] [G loss: 1.000073]\n",
      "65 [D loss: 0.999975] [G loss: 1.000068]\n",
      "66 [D loss: 0.999992] [G loss: 1.000074]\n",
      "67 [D loss: 0.999980] [G loss: 1.000071]\n",
      "68 [D loss: 0.999993] [G loss: 1.000070]\n",
      "69 [D loss: 0.999986] [G loss: 1.000081]\n",
      "70 [D loss: 0.999986] [G loss: 1.000084]\n",
      "71 [D loss: 0.999981] [G loss: 1.000091]\n",
      "72 [D loss: 0.999978] [G loss: 1.000105]\n",
      "73 [D loss: 0.999970] [G loss: 1.000087]\n",
      "74 [D loss: 0.999979] [G loss: 1.000114]\n",
      "75 [D loss: 0.999977] [G loss: 1.000097]\n",
      "76 [D loss: 0.999981] [G loss: 1.000088]\n",
      "77 [D loss: 0.999981] [G loss: 1.000081]\n",
      "78 [D loss: 0.999966] [G loss: 1.000100]\n",
      "79 [D loss: 0.999961] [G loss: 1.000094]\n",
      "80 [D loss: 0.999952] [G loss: 1.000119]\n",
      "81 [D loss: 0.999963] [G loss: 1.000095]\n",
      "82 [D loss: 0.999984] [G loss: 1.000106]\n",
      "83 [D loss: 0.999967] [G loss: 1.000116]\n",
      "84 [D loss: 0.999964] [G loss: 1.000103]\n",
      "85 [D loss: 0.999962] [G loss: 1.000107]\n",
      "86 [D loss: 0.999969] [G loss: 1.000091]\n",
      "87 [D loss: 0.999966] [G loss: 1.000100]\n",
      "88 [D loss: 0.999964] [G loss: 1.000114]\n",
      "89 [D loss: 0.999972] [G loss: 1.000107]\n",
      "90 [D loss: 0.999961] [G loss: 1.000120]\n",
      "91 [D loss: 0.999966] [G loss: 1.000109]\n",
      "92 [D loss: 0.999973] [G loss: 1.000111]\n",
      "93 [D loss: 0.999975] [G loss: 1.000106]\n",
      "94 [D loss: 0.999964] [G loss: 1.000115]\n",
      "95 [D loss: 0.999975] [G loss: 1.000108]\n",
      "96 [D loss: 0.999973] [G loss: 1.000110]\n",
      "97 [D loss: 0.999969] [G loss: 1.000103]\n",
      "98 [D loss: 0.999969] [G loss: 1.000094]\n",
      "99 [D loss: 0.999958] [G loss: 1.000107]\n",
      "100 [D loss: 0.999959] [G loss: 1.000091]\n",
      "101 [D loss: 0.999959] [G loss: 1.000109]\n",
      "102 [D loss: 0.999969] [G loss: 1.000101]\n",
      "103 [D loss: 0.999970] [G loss: 1.000091]\n",
      "104 [D loss: 0.999971] [G loss: 1.000089]\n",
      "105 [D loss: 0.999960] [G loss: 1.000108]\n",
      "106 [D loss: 0.999970] [G loss: 1.000095]\n",
      "107 [D loss: 0.999955] [G loss: 1.000092]\n",
      "108 [D loss: 0.999969] [G loss: 1.000096]\n",
      "109 [D loss: 0.999966] [G loss: 1.000091]\n",
      "110 [D loss: 0.999968] [G loss: 1.000086]\n",
      "111 [D loss: 0.999962] [G loss: 1.000090]\n",
      "112 [D loss: 0.999963] [G loss: 1.000088]\n",
      "113 [D loss: 0.999981] [G loss: 1.000088]\n",
      "114 [D loss: 0.999966] [G loss: 1.000079]\n",
      "115 [D loss: 0.999963] [G loss: 1.000074]\n",
      "116 [D loss: 0.999968] [G loss: 1.000095]\n",
      "117 [D loss: 0.999963] [G loss: 1.000069]\n",
      "118 [D loss: 0.999966] [G loss: 1.000084]\n",
      "119 [D loss: 0.999970] [G loss: 1.000095]\n",
      "120 [D loss: 0.999963] [G loss: 1.000086]\n",
      "121 [D loss: 0.999967] [G loss: 1.000093]\n",
      "122 [D loss: 0.999968] [G loss: 1.000083]\n",
      "123 [D loss: 0.999971] [G loss: 1.000093]\n",
      "124 [D loss: 0.999972] [G loss: 1.000081]\n",
      "125 [D loss: 0.999963] [G loss: 1.000081]\n",
      "126 [D loss: 0.999971] [G loss: 1.000075]\n",
      "127 [D loss: 0.999967] [G loss: 1.000081]\n",
      "128 [D loss: 0.999971] [G loss: 1.000083]\n",
      "129 [D loss: 0.999966] [G loss: 1.000083]\n",
      "130 [D loss: 0.999970] [G loss: 1.000079]\n",
      "131 [D loss: 0.999965] [G loss: 1.000074]\n",
      "132 [D loss: 0.999967] [G loss: 1.000071]\n",
      "133 [D loss: 0.999964] [G loss: 1.000074]\n",
      "134 [D loss: 0.999969] [G loss: 1.000077]\n",
      "135 [D loss: 0.999962] [G loss: 1.000076]\n",
      "136 [D loss: 0.999973] [G loss: 1.000068]\n",
      "137 [D loss: 0.999969] [G loss: 1.000072]\n",
      "138 [D loss: 0.999969] [G loss: 1.000070]\n",
      "139 [D loss: 0.999969] [G loss: 1.000067]\n",
      "140 [D loss: 0.999967] [G loss: 1.000077]\n",
      "141 [D loss: 0.999969] [G loss: 1.000072]\n",
      "142 [D loss: 0.999961] [G loss: 1.000076]\n",
      "143 [D loss: 0.999972] [G loss: 1.000077]\n",
      "144 [D loss: 0.999962] [G loss: 1.000075]\n",
      "145 [D loss: 0.999967] [G loss: 1.000075]\n",
      "146 [D loss: 0.999970] [G loss: 1.000081]\n",
      "147 [D loss: 0.999976] [G loss: 1.000067]\n",
      "148 [D loss: 0.999969] [G loss: 1.000072]\n",
      "149 [D loss: 0.999966] [G loss: 1.000076]\n",
      "150 [D loss: 0.999969] [G loss: 1.000061]\n",
      "151 [D loss: 0.999964] [G loss: 1.000065]\n",
      "152 [D loss: 0.999971] [G loss: 1.000059]\n",
      "153 [D loss: 0.999961] [G loss: 1.000070]\n",
      "154 [D loss: 0.999976] [G loss: 1.000076]\n",
      "155 [D loss: 0.999967] [G loss: 1.000072]\n",
      "156 [D loss: 0.999972] [G loss: 1.000063]\n",
      "157 [D loss: 0.999957] [G loss: 1.000069]\n",
      "158 [D loss: 0.999964] [G loss: 1.000053]\n",
      "159 [D loss: 0.999960] [G loss: 1.000059]\n",
      "160 [D loss: 0.999967] [G loss: 1.000066]\n",
      "161 [D loss: 0.999972] [G loss: 1.000074]\n",
      "162 [D loss: 0.999971] [G loss: 1.000080]\n",
      "163 [D loss: 0.999972] [G loss: 1.000066]\n",
      "164 [D loss: 0.999970] [G loss: 1.000081]\n",
      "165 [D loss: 0.999963] [G loss: 1.000067]\n",
      "166 [D loss: 0.999970] [G loss: 1.000043]\n",
      "167 [D loss: 0.999966] [G loss: 1.000066]\n",
      "168 [D loss: 0.999965] [G loss: 1.000075]\n",
      "169 [D loss: 0.999970] [G loss: 1.000073]\n",
      "170 [D loss: 0.999963] [G loss: 1.000065]\n",
      "171 [D loss: 0.999975] [G loss: 1.000059]\n",
      "172 [D loss: 0.999971] [G loss: 1.000061]\n",
      "173 [D loss: 0.999973] [G loss: 1.000066]\n",
      "174 [D loss: 0.999981] [G loss: 1.000048]\n",
      "175 [D loss: 0.999966] [G loss: 1.000071]\n",
      "176 [D loss: 0.999969] [G loss: 1.000069]\n",
      "177 [D loss: 0.999966] [G loss: 1.000064]\n",
      "178 [D loss: 0.999968] [G loss: 1.000075]\n",
      "179 [D loss: 0.999968] [G loss: 1.000061]\n",
      "180 [D loss: 0.999958] [G loss: 1.000061]\n",
      "181 [D loss: 0.999967] [G loss: 1.000070]\n",
      "182 [D loss: 0.999970] [G loss: 1.000058]\n",
      "183 [D loss: 0.999965] [G loss: 1.000080]\n",
      "184 [D loss: 0.999965] [G loss: 1.000069]\n",
      "185 [D loss: 0.999969] [G loss: 1.000063]\n",
      "186 [D loss: 0.999965] [G loss: 1.000075]\n",
      "187 [D loss: 0.999968] [G loss: 1.000075]\n",
      "188 [D loss: 0.999973] [G loss: 1.000058]\n",
      "189 [D loss: 0.999960] [G loss: 1.000055]\n",
      "190 [D loss: 0.999968] [G loss: 1.000071]\n",
      "191 [D loss: 0.999969] [G loss: 1.000069]\n",
      "192 [D loss: 0.999972] [G loss: 1.000069]\n",
      "193 [D loss: 0.999968] [G loss: 1.000062]\n",
      "194 [D loss: 0.999964] [G loss: 1.000059]\n",
      "195 [D loss: 0.999973] [G loss: 1.000064]\n",
      "196 [D loss: 0.999970] [G loss: 1.000068]\n",
      "197 [D loss: 0.999970] [G loss: 1.000079]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 [D loss: 0.999972] [G loss: 1.000075]\n",
      "199 [D loss: 0.999970] [G loss: 1.000068]\n",
      "200 [D loss: 0.999970] [G loss: 1.000065]\n",
      "201 [D loss: 0.999980] [G loss: 1.000062]\n",
      "202 [D loss: 0.999968] [G loss: 1.000070]\n",
      "203 [D loss: 0.999974] [G loss: 1.000051]\n",
      "204 [D loss: 0.999960] [G loss: 1.000076]\n",
      "205 [D loss: 0.999968] [G loss: 1.000078]\n",
      "206 [D loss: 0.999961] [G loss: 1.000081]\n",
      "207 [D loss: 0.999968] [G loss: 1.000060]\n",
      "208 [D loss: 0.999960] [G loss: 1.000065]\n",
      "209 [D loss: 0.999967] [G loss: 1.000073]\n",
      "210 [D loss: 0.999967] [G loss: 1.000062]\n",
      "211 [D loss: 0.999969] [G loss: 1.000070]\n",
      "212 [D loss: 0.999970] [G loss: 1.000063]\n",
      "213 [D loss: 0.999963] [G loss: 1.000067]\n",
      "214 [D loss: 0.999969] [G loss: 1.000067]\n",
      "215 [D loss: 0.999974] [G loss: 1.000070]\n",
      "216 [D loss: 0.999972] [G loss: 1.000062]\n",
      "217 [D loss: 0.999968] [G loss: 1.000071]\n",
      "218 [D loss: 0.999970] [G loss: 1.000070]\n",
      "219 [D loss: 0.999967] [G loss: 1.000079]\n",
      "220 [D loss: 0.999971] [G loss: 1.000071]\n",
      "221 [D loss: 0.999975] [G loss: 1.000068]\n",
      "222 [D loss: 0.999972] [G loss: 1.000076]\n",
      "223 [D loss: 0.999973] [G loss: 1.000074]\n",
      "224 [D loss: 0.999963] [G loss: 1.000083]\n",
      "225 [D loss: 0.999967] [G loss: 1.000069]\n",
      "226 [D loss: 0.999970] [G loss: 1.000070]\n",
      "227 [D loss: 0.999971] [G loss: 1.000062]\n",
      "228 [D loss: 0.999966] [G loss: 1.000063]\n",
      "229 [D loss: 0.999967] [G loss: 1.000070]\n",
      "230 [D loss: 0.999965] [G loss: 1.000073]\n",
      "231 [D loss: 0.999972] [G loss: 1.000078]\n",
      "232 [D loss: 0.999965] [G loss: 1.000066]\n",
      "233 [D loss: 0.999971] [G loss: 1.000071]\n",
      "234 [D loss: 0.999975] [G loss: 1.000074]\n",
      "235 [D loss: 0.999966] [G loss: 1.000060]\n",
      "236 [D loss: 0.999978] [G loss: 1.000079]\n",
      "237 [D loss: 0.999977] [G loss: 1.000071]\n",
      "238 [D loss: 0.999963] [G loss: 1.000062]\n",
      "239 [D loss: 0.999962] [G loss: 1.000064]\n",
      "240 [D loss: 0.999975] [G loss: 1.000066]\n",
      "241 [D loss: 0.999970] [G loss: 1.000074]\n",
      "242 [D loss: 0.999974] [G loss: 1.000073]\n",
      "243 [D loss: 0.999967] [G loss: 1.000073]\n",
      "244 [D loss: 0.999969] [G loss: 1.000063]\n",
      "245 [D loss: 0.999970] [G loss: 1.000070]\n",
      "246 [D loss: 0.999969] [G loss: 1.000058]\n",
      "247 [D loss: 0.999972] [G loss: 1.000060]\n",
      "248 [D loss: 0.999972] [G loss: 1.000078]\n",
      "249 [D loss: 0.999962] [G loss: 1.000072]\n",
      "250 [D loss: 0.999961] [G loss: 1.000067]\n",
      "251 [D loss: 0.999969] [G loss: 1.000073]\n",
      "252 [D loss: 0.999974] [G loss: 1.000059]\n",
      "253 [D loss: 0.999969] [G loss: 1.000073]\n",
      "254 [D loss: 0.999971] [G loss: 1.000066]\n",
      "255 [D loss: 0.999973] [G loss: 1.000060]\n",
      "256 [D loss: 0.999967] [G loss: 1.000068]\n",
      "257 [D loss: 0.999958] [G loss: 1.000074]\n",
      "258 [D loss: 0.999950] [G loss: 1.000061]\n",
      "259 [D loss: 0.999964] [G loss: 1.000070]\n",
      "260 [D loss: 0.999975] [G loss: 1.000058]\n",
      "261 [D loss: 0.999971] [G loss: 1.000055]\n",
      "262 [D loss: 0.999960] [G loss: 1.000065]\n",
      "263 [D loss: 0.999960] [G loss: 1.000063]\n",
      "264 [D loss: 0.999972] [G loss: 1.000073]\n",
      "265 [D loss: 0.999970] [G loss: 1.000073]\n",
      "266 [D loss: 0.999967] [G loss: 1.000071]\n",
      "267 [D loss: 0.999966] [G loss: 1.000056]\n",
      "268 [D loss: 0.999943] [G loss: 1.000070]\n",
      "269 [D loss: 0.999967] [G loss: 1.000072]\n",
      "270 [D loss: 0.999958] [G loss: 1.000068]\n",
      "271 [D loss: 0.999963] [G loss: 1.000057]\n",
      "272 [D loss: 0.999966] [G loss: 1.000056]\n",
      "273 [D loss: 0.999954] [G loss: 1.000060]\n",
      "274 [D loss: 0.999971] [G loss: 1.000072]\n",
      "275 [D loss: 0.999963] [G loss: 1.000062]\n",
      "276 [D loss: 0.999959] [G loss: 1.000068]\n",
      "277 [D loss: 0.999974] [G loss: 1.000065]\n",
      "278 [D loss: 0.999964] [G loss: 1.000057]\n",
      "279 [D loss: 0.999966] [G loss: 1.000069]\n",
      "280 [D loss: 0.999970] [G loss: 1.000061]\n",
      "281 [D loss: 0.999962] [G loss: 1.000053]\n",
      "282 [D loss: 0.999967] [G loss: 1.000066]\n",
      "283 [D loss: 0.999965] [G loss: 1.000066]\n",
      "284 [D loss: 0.999968] [G loss: 1.000062]\n",
      "285 [D loss: 0.999963] [G loss: 1.000048]\n",
      "286 [D loss: 0.999964] [G loss: 1.000069]\n",
      "287 [D loss: 0.999976] [G loss: 1.000079]\n",
      "288 [D loss: 0.999971] [G loss: 1.000071]\n",
      "289 [D loss: 0.999969] [G loss: 1.000062]\n",
      "290 [D loss: 0.999969] [G loss: 1.000069]\n",
      "291 [D loss: 0.999960] [G loss: 1.000064]\n",
      "292 [D loss: 0.999981] [G loss: 1.000051]\n",
      "293 [D loss: 0.999965] [G loss: 1.000070]\n",
      "294 [D loss: 0.999959] [G loss: 1.000062]\n",
      "295 [D loss: 0.999971] [G loss: 1.000069]\n",
      "296 [D loss: 0.999976] [G loss: 1.000040]\n",
      "297 [D loss: 0.999963] [G loss: 1.000063]\n",
      "298 [D loss: 0.999965] [G loss: 1.000070]\n",
      "299 [D loss: 0.999964] [G loss: 1.000072]\n",
      "300 [D loss: 0.999968] [G loss: 1.000069]\n",
      "301 [D loss: 0.999965] [G loss: 1.000073]\n",
      "302 [D loss: 0.999976] [G loss: 1.000068]\n",
      "303 [D loss: 0.999965] [G loss: 1.000059]\n",
      "304 [D loss: 0.999977] [G loss: 1.000066]\n",
      "305 [D loss: 0.999970] [G loss: 1.000060]\n",
      "306 [D loss: 0.999967] [G loss: 1.000060]\n",
      "307 [D loss: 0.999982] [G loss: 1.000066]\n",
      "308 [D loss: 0.999966] [G loss: 1.000067]\n",
      "309 [D loss: 0.999965] [G loss: 1.000073]\n",
      "310 [D loss: 0.999969] [G loss: 1.000075]\n",
      "311 [D loss: 0.999972] [G loss: 1.000053]\n",
      "312 [D loss: 0.999967] [G loss: 1.000073]\n",
      "313 [D loss: 0.999965] [G loss: 1.000070]\n",
      "314 [D loss: 0.999961] [G loss: 1.000055]\n",
      "315 [D loss: 0.999979] [G loss: 1.000059]\n",
      "316 [D loss: 0.999970] [G loss: 1.000071]\n",
      "317 [D loss: 0.999966] [G loss: 1.000059]\n",
      "318 [D loss: 0.999960] [G loss: 1.000065]\n",
      "319 [D loss: 0.999971] [G loss: 1.000063]\n",
      "320 [D loss: 0.999967] [G loss: 1.000069]\n",
      "321 [D loss: 0.999968] [G loss: 1.000059]\n",
      "322 [D loss: 0.999966] [G loss: 1.000067]\n",
      "323 [D loss: 0.999971] [G loss: 1.000073]\n",
      "324 [D loss: 0.999968] [G loss: 1.000067]\n",
      "325 [D loss: 0.999967] [G loss: 1.000067]\n",
      "326 [D loss: 0.999964] [G loss: 1.000058]\n",
      "327 [D loss: 0.999976] [G loss: 1.000071]\n",
      "328 [D loss: 0.999971] [G loss: 1.000065]\n",
      "329 [D loss: 0.999966] [G loss: 1.000070]\n",
      "330 [D loss: 0.999966] [G loss: 1.000063]\n",
      "331 [D loss: 0.999968] [G loss: 1.000062]\n",
      "332 [D loss: 0.999968] [G loss: 1.000071]\n",
      "333 [D loss: 0.999974] [G loss: 1.000060]\n",
      "334 [D loss: 0.999971] [G loss: 1.000066]\n",
      "335 [D loss: 0.999979] [G loss: 1.000071]\n",
      "336 [D loss: 0.999982] [G loss: 1.000073]\n",
      "337 [D loss: 0.999965] [G loss: 1.000061]\n",
      "338 [D loss: 0.999968] [G loss: 1.000060]\n",
      "339 [D loss: 0.999969] [G loss: 1.000064]\n",
      "340 [D loss: 0.999971] [G loss: 1.000060]\n",
      "341 [D loss: 0.999978] [G loss: 1.000067]\n",
      "342 [D loss: 0.999957] [G loss: 1.000063]\n",
      "343 [D loss: 0.999963] [G loss: 1.000061]\n",
      "344 [D loss: 0.999972] [G loss: 1.000068]\n",
      "345 [D loss: 0.999974] [G loss: 1.000070]\n",
      "346 [D loss: 0.999963] [G loss: 1.000056]\n",
      "347 [D loss: 0.999968] [G loss: 1.000061]\n",
      "348 [D loss: 0.999968] [G loss: 1.000071]\n",
      "349 [D loss: 0.999963] [G loss: 1.000065]\n",
      "350 [D loss: 0.999972] [G loss: 1.000077]\n",
      "351 [D loss: 0.999966] [G loss: 1.000061]\n",
      "352 [D loss: 0.999968] [G loss: 1.000065]\n",
      "353 [D loss: 0.999965] [G loss: 1.000055]\n",
      "354 [D loss: 0.999970] [G loss: 1.000058]\n",
      "355 [D loss: 0.999968] [G loss: 1.000064]\n",
      "356 [D loss: 0.999971] [G loss: 1.000056]\n",
      "357 [D loss: 0.999959] [G loss: 1.000062]\n",
      "358 [D loss: 0.999970] [G loss: 1.000059]\n",
      "359 [D loss: 0.999979] [G loss: 1.000064]\n",
      "360 [D loss: 0.999966] [G loss: 1.000054]\n",
      "361 [D loss: 0.999980] [G loss: 1.000064]\n",
      "362 [D loss: 0.999956] [G loss: 1.000057]\n",
      "363 [D loss: 0.999971] [G loss: 1.000069]\n",
      "364 [D loss: 0.999963] [G loss: 1.000048]\n",
      "365 [D loss: 0.999973] [G loss: 1.000057]\n",
      "366 [D loss: 0.999976] [G loss: 1.000060]\n",
      "367 [D loss: 0.999968] [G loss: 1.000074]\n",
      "368 [D loss: 0.999961] [G loss: 1.000056]\n",
      "369 [D loss: 0.999966] [G loss: 1.000061]\n",
      "370 [D loss: 0.999963] [G loss: 1.000065]\n",
      "371 [D loss: 0.999969] [G loss: 1.000072]\n",
      "372 [D loss: 0.999962] [G loss: 1.000059]\n",
      "373 [D loss: 0.999969] [G loss: 1.000061]\n",
      "374 [D loss: 0.999971] [G loss: 1.000066]\n",
      "375 [D loss: 0.999975] [G loss: 1.000061]\n",
      "376 [D loss: 0.999969] [G loss: 1.000062]\n",
      "377 [D loss: 0.999967] [G loss: 1.000069]\n",
      "378 [D loss: 0.999971] [G loss: 1.000067]\n",
      "379 [D loss: 0.999970] [G loss: 1.000063]\n",
      "380 [D loss: 0.999968] [G loss: 1.000065]\n",
      "381 [D loss: 0.999973] [G loss: 1.000069]\n",
      "382 [D loss: 0.999968] [G loss: 1.000061]\n",
      "383 [D loss: 0.999964] [G loss: 1.000059]\n",
      "384 [D loss: 0.999971] [G loss: 1.000078]\n",
      "385 [D loss: 0.999972] [G loss: 1.000063]\n",
      "386 [D loss: 0.999973] [G loss: 1.000069]\n",
      "387 [D loss: 0.999966] [G loss: 1.000068]\n",
      "388 [D loss: 0.999970] [G loss: 1.000070]\n",
      "389 [D loss: 0.999968] [G loss: 1.000063]\n",
      "390 [D loss: 0.999965] [G loss: 1.000072]\n",
      "391 [D loss: 0.999972] [G loss: 1.000068]\n",
      "392 [D loss: 0.999971] [G loss: 1.000061]\n",
      "393 [D loss: 0.999967] [G loss: 1.000069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394 [D loss: 0.999973] [G loss: 1.000059]\n",
      "395 [D loss: 0.999970] [G loss: 1.000071]\n",
      "396 [D loss: 0.999964] [G loss: 1.000068]\n",
      "397 [D loss: 0.999971] [G loss: 1.000062]\n",
      "398 [D loss: 0.999970] [G loss: 1.000072]\n",
      "399 [D loss: 0.999965] [G loss: 1.000062]\n",
      "400 [D loss: 0.999971] [G loss: 1.000054]\n",
      "401 [D loss: 0.999960] [G loss: 1.000072]\n",
      "402 [D loss: 0.999969] [G loss: 1.000062]\n",
      "403 [D loss: 0.999976] [G loss: 1.000074]\n",
      "404 [D loss: 0.999975] [G loss: 1.000063]\n",
      "405 [D loss: 0.999961] [G loss: 1.000046]\n",
      "406 [D loss: 0.999973] [G loss: 1.000061]\n",
      "407 [D loss: 0.999969] [G loss: 1.000061]\n",
      "408 [D loss: 0.999957] [G loss: 1.000066]\n",
      "409 [D loss: 0.999964] [G loss: 1.000068]\n",
      "410 [D loss: 0.999972] [G loss: 1.000048]\n",
      "411 [D loss: 0.999966] [G loss: 1.000064]\n",
      "412 [D loss: 0.999964] [G loss: 1.000065]\n",
      "413 [D loss: 0.999963] [G loss: 1.000050]\n",
      "414 [D loss: 0.999970] [G loss: 1.000069]\n",
      "415 [D loss: 0.999969] [G loss: 1.000070]\n",
      "416 [D loss: 0.999967] [G loss: 1.000067]\n",
      "417 [D loss: 0.999967] [G loss: 1.000062]\n",
      "418 [D loss: 0.999960] [G loss: 1.000059]\n",
      "419 [D loss: 0.999968] [G loss: 1.000054]\n",
      "420 [D loss: 0.999967] [G loss: 1.000057]\n",
      "421 [D loss: 0.999977] [G loss: 1.000062]\n",
      "422 [D loss: 0.999969] [G loss: 1.000061]\n",
      "423 [D loss: 0.999969] [G loss: 1.000073]\n",
      "424 [D loss: 0.999965] [G loss: 1.000056]\n",
      "425 [D loss: 0.999973] [G loss: 1.000066]\n",
      "426 [D loss: 0.999962] [G loss: 1.000062]\n",
      "427 [D loss: 0.999974] [G loss: 1.000063]\n",
      "428 [D loss: 0.999976] [G loss: 1.000058]\n",
      "429 [D loss: 0.999968] [G loss: 1.000071]\n",
      "430 [D loss: 0.999967] [G loss: 1.000064]\n",
      "431 [D loss: 0.999971] [G loss: 1.000064]\n",
      "432 [D loss: 0.999967] [G loss: 1.000058]\n",
      "433 [D loss: 0.999976] [G loss: 1.000066]\n",
      "434 [D loss: 0.999970] [G loss: 1.000061]\n",
      "435 [D loss: 0.999971] [G loss: 1.000064]\n",
      "436 [D loss: 0.999968] [G loss: 1.000071]\n",
      "437 [D loss: 0.999976] [G loss: 1.000070]\n",
      "438 [D loss: 0.999975] [G loss: 1.000060]\n",
      "439 [D loss: 0.999967] [G loss: 1.000068]\n",
      "440 [D loss: 0.999976] [G loss: 1.000056]\n",
      "441 [D loss: 0.999960] [G loss: 1.000070]\n",
      "442 [D loss: 0.999969] [G loss: 1.000069]\n",
      "443 [D loss: 0.999966] [G loss: 1.000061]\n",
      "444 [D loss: 0.999965] [G loss: 1.000059]\n",
      "445 [D loss: 0.999968] [G loss: 1.000056]\n",
      "446 [D loss: 0.999973] [G loss: 1.000058]\n",
      "447 [D loss: 0.999961] [G loss: 1.000073]\n",
      "448 [D loss: 0.999969] [G loss: 1.000064]\n",
      "449 [D loss: 0.999970] [G loss: 1.000052]\n",
      "450 [D loss: 0.999968] [G loss: 1.000065]\n",
      "451 [D loss: 0.999970] [G loss: 1.000062]\n",
      "452 [D loss: 0.999975] [G loss: 1.000065]\n",
      "453 [D loss: 0.999969] [G loss: 1.000069]\n",
      "454 [D loss: 0.999968] [G loss: 1.000071]\n",
      "455 [D loss: 0.999965] [G loss: 1.000069]\n",
      "456 [D loss: 0.999963] [G loss: 1.000073]\n",
      "457 [D loss: 0.999973] [G loss: 1.000073]\n",
      "458 [D loss: 0.999967] [G loss: 1.000069]\n",
      "459 [D loss: 0.999965] [G loss: 1.000060]\n",
      "460 [D loss: 0.999975] [G loss: 1.000058]\n",
      "461 [D loss: 0.999973] [G loss: 1.000070]\n",
      "462 [D loss: 0.999973] [G loss: 1.000064]\n",
      "463 [D loss: 0.999968] [G loss: 1.000064]\n",
      "464 [D loss: 0.999970] [G loss: 1.000073]\n",
      "465 [D loss: 0.999971] [G loss: 1.000074]\n",
      "466 [D loss: 0.999967] [G loss: 1.000065]\n",
      "467 [D loss: 0.999974] [G loss: 1.000073]\n",
      "468 [D loss: 0.999972] [G loss: 1.000072]\n",
      "469 [D loss: 0.999965] [G loss: 1.000069]\n",
      "470 [D loss: 0.999968] [G loss: 1.000060]\n",
      "471 [D loss: 0.999974] [G loss: 1.000068]\n",
      "472 [D loss: 0.999967] [G loss: 1.000073]\n",
      "473 [D loss: 0.999972] [G loss: 1.000072]\n",
      "474 [D loss: 0.999966] [G loss: 1.000076]\n",
      "475 [D loss: 0.999967] [G loss: 1.000067]\n",
      "476 [D loss: 0.999972] [G loss: 1.000067]\n",
      "477 [D loss: 0.999972] [G loss: 1.000060]\n",
      "478 [D loss: 0.999971] [G loss: 1.000069]\n",
      "479 [D loss: 0.999975] [G loss: 1.000065]\n",
      "480 [D loss: 0.999967] [G loss: 1.000063]\n",
      "481 [D loss: 0.999969] [G loss: 1.000069]\n",
      "482 [D loss: 0.999966] [G loss: 1.000062]\n",
      "483 [D loss: 0.999965] [G loss: 1.000056]\n",
      "484 [D loss: 0.999973] [G loss: 1.000063]\n",
      "485 [D loss: 0.999973] [G loss: 1.000065]\n",
      "486 [D loss: 0.999975] [G loss: 1.000066]\n",
      "487 [D loss: 0.999968] [G loss: 1.000071]\n",
      "488 [D loss: 0.999968] [G loss: 1.000069]\n",
      "489 [D loss: 0.999976] [G loss: 1.000066]\n",
      "490 [D loss: 0.999969] [G loss: 1.000061]\n",
      "491 [D loss: 0.999973] [G loss: 1.000075]\n",
      "492 [D loss: 0.999976] [G loss: 1.000072]\n",
      "493 [D loss: 0.999973] [G loss: 1.000065]\n",
      "494 [D loss: 0.999968] [G loss: 1.000067]\n",
      "495 [D loss: 0.999976] [G loss: 1.000059]\n",
      "496 [D loss: 0.999968] [G loss: 1.000069]\n",
      "497 [D loss: 0.999974] [G loss: 1.000066]\n",
      "498 [D loss: 0.999964] [G loss: 1.000074]\n",
      "499 [D loss: 0.999971] [G loss: 1.000059]\n",
      "500 [D loss: 0.999968] [G loss: 1.000070]\n",
      "501 [D loss: 0.999969] [G loss: 1.000046]\n",
      "502 [D loss: 0.999960] [G loss: 1.000060]\n",
      "503 [D loss: 0.999980] [G loss: 1.000067]\n",
      "504 [D loss: 0.999967] [G loss: 1.000057]\n",
      "505 [D loss: 0.999973] [G loss: 1.000067]\n",
      "506 [D loss: 0.999976] [G loss: 1.000063]\n",
      "507 [D loss: 0.999968] [G loss: 1.000048]\n",
      "508 [D loss: 0.999971] [G loss: 1.000061]\n",
      "509 [D loss: 0.999975] [G loss: 1.000068]\n",
      "510 [D loss: 0.999969] [G loss: 1.000071]\n",
      "511 [D loss: 0.999968] [G loss: 1.000069]\n",
      "512 [D loss: 0.999968] [G loss: 1.000067]\n",
      "513 [D loss: 0.999967] [G loss: 1.000057]\n",
      "514 [D loss: 0.999960] [G loss: 1.000061]\n",
      "515 [D loss: 0.999971] [G loss: 1.000070]\n",
      "516 [D loss: 0.999969] [G loss: 1.000059]\n",
      "517 [D loss: 0.999964] [G loss: 1.000065]\n",
      "518 [D loss: 0.999969] [G loss: 1.000060]\n",
      "519 [D loss: 0.999975] [G loss: 1.000063]\n",
      "520 [D loss: 0.999971] [G loss: 1.000069]\n",
      "521 [D loss: 0.999966] [G loss: 1.000054]\n",
      "522 [D loss: 0.999969] [G loss: 1.000058]\n",
      "523 [D loss: 0.999974] [G loss: 1.000070]\n",
      "524 [D loss: 0.999969] [G loss: 1.000064]\n",
      "525 [D loss: 0.999960] [G loss: 1.000072]\n",
      "526 [D loss: 0.999971] [G loss: 1.000065]\n",
      "527 [D loss: 0.999972] [G loss: 1.000057]\n",
      "528 [D loss: 0.999972] [G loss: 1.000066]\n",
      "529 [D loss: 0.999968] [G loss: 1.000063]\n",
      "530 [D loss: 0.999968] [G loss: 1.000067]\n",
      "531 [D loss: 0.999971] [G loss: 1.000065]\n",
      "532 [D loss: 0.999970] [G loss: 1.000067]\n",
      "533 [D loss: 0.999970] [G loss: 1.000067]\n",
      "534 [D loss: 0.999968] [G loss: 1.000069]\n",
      "535 [D loss: 0.999963] [G loss: 1.000055]\n",
      "536 [D loss: 0.999975] [G loss: 1.000071]\n",
      "537 [D loss: 0.999975] [G loss: 1.000056]\n",
      "538 [D loss: 0.999954] [G loss: 1.000061]\n",
      "539 [D loss: 0.999969] [G loss: 1.000066]\n",
      "540 [D loss: 0.999969] [G loss: 1.000069]\n",
      "541 [D loss: 0.999964] [G loss: 1.000053]\n",
      "542 [D loss: 0.999974] [G loss: 1.000057]\n",
      "543 [D loss: 0.999979] [G loss: 1.000074]\n",
      "544 [D loss: 0.999958] [G loss: 1.000082]\n",
      "545 [D loss: 0.999975] [G loss: 1.000068]\n",
      "546 [D loss: 0.999970] [G loss: 1.000062]\n",
      "547 [D loss: 0.999970] [G loss: 1.000067]\n",
      "548 [D loss: 0.999972] [G loss: 1.000057]\n",
      "549 [D loss: 0.999984] [G loss: 1.000061]\n",
      "550 [D loss: 0.999967] [G loss: 1.000049]\n",
      "551 [D loss: 0.999981] [G loss: 1.000068]\n",
      "552 [D loss: 0.999965] [G loss: 1.000058]\n",
      "553 [D loss: 0.999969] [G loss: 1.000074]\n",
      "554 [D loss: 0.999964] [G loss: 1.000061]\n",
      "555 [D loss: 0.999964] [G loss: 1.000060]\n",
      "556 [D loss: 0.999962] [G loss: 1.000082]\n",
      "557 [D loss: 0.999970] [G loss: 1.000046]\n",
      "558 [D loss: 0.999963] [G loss: 1.000058]\n",
      "559 [D loss: 0.999968] [G loss: 1.000064]\n",
      "560 [D loss: 0.999966] [G loss: 1.000074]\n",
      "561 [D loss: 0.999964] [G loss: 1.000065]\n",
      "562 [D loss: 0.999971] [G loss: 1.000069]\n",
      "563 [D loss: 0.999977] [G loss: 1.000058]\n",
      "564 [D loss: 0.999967] [G loss: 1.000068]\n",
      "565 [D loss: 0.999967] [G loss: 1.000056]\n",
      "566 [D loss: 0.999973] [G loss: 1.000064]\n",
      "567 [D loss: 0.999972] [G loss: 1.000069]\n",
      "568 [D loss: 0.999973] [G loss: 1.000060]\n",
      "569 [D loss: 0.999971] [G loss: 1.000062]\n",
      "570 [D loss: 0.999971] [G loss: 1.000075]\n",
      "571 [D loss: 0.999966] [G loss: 1.000060]\n",
      "572 [D loss: 0.999968] [G loss: 1.000061]\n",
      "573 [D loss: 0.999969] [G loss: 1.000061]\n",
      "574 [D loss: 0.999967] [G loss: 1.000056]\n",
      "575 [D loss: 0.999976] [G loss: 1.000067]\n",
      "576 [D loss: 0.999978] [G loss: 1.000063]\n",
      "577 [D loss: 0.999964] [G loss: 1.000060]\n",
      "578 [D loss: 0.999970] [G loss: 1.000066]\n",
      "579 [D loss: 0.999971] [G loss: 1.000070]\n",
      "580 [D loss: 0.999962] [G loss: 1.000077]\n",
      "581 [D loss: 0.999967] [G loss: 1.000061]\n",
      "582 [D loss: 0.999973] [G loss: 1.000069]\n",
      "583 [D loss: 0.999969] [G loss: 1.000066]\n",
      "584 [D loss: 0.999965] [G loss: 1.000074]\n",
      "585 [D loss: 0.999970] [G loss: 1.000071]\n",
      "586 [D loss: 0.999968] [G loss: 1.000061]\n",
      "587 [D loss: 0.999970] [G loss: 1.000066]\n",
      "588 [D loss: 0.999968] [G loss: 1.000062]\n",
      "589 [D loss: 0.999968] [G loss: 1.000064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590 [D loss: 0.999976] [G loss: 1.000065]\n",
      "591 [D loss: 0.999971] [G loss: 1.000055]\n",
      "592 [D loss: 0.999969] [G loss: 1.000064]\n",
      "593 [D loss: 0.999977] [G loss: 1.000062]\n",
      "594 [D loss: 0.999978] [G loss: 1.000058]\n",
      "595 [D loss: 0.999968] [G loss: 1.000069]\n",
      "596 [D loss: 0.999968] [G loss: 1.000064]\n",
      "597 [D loss: 0.999963] [G loss: 1.000069]\n",
      "598 [D loss: 0.999970] [G loss: 1.000059]\n",
      "599 [D loss: 0.999972] [G loss: 1.000066]\n",
      "600 [D loss: 0.999972] [G loss: 1.000064]\n",
      "601 [D loss: 0.999975] [G loss: 1.000069]\n",
      "602 [D loss: 0.999968] [G loss: 1.000064]\n",
      "603 [D loss: 0.999972] [G loss: 1.000062]\n",
      "604 [D loss: 0.999973] [G loss: 1.000065]\n",
      "605 [D loss: 0.999968] [G loss: 1.000068]\n",
      "606 [D loss: 0.999969] [G loss: 1.000075]\n",
      "607 [D loss: 0.999974] [G loss: 1.000064]\n",
      "608 [D loss: 0.999969] [G loss: 1.000064]\n",
      "609 [D loss: 0.999968] [G loss: 1.000068]\n",
      "610 [D loss: 0.999964] [G loss: 1.000064]\n",
      "611 [D loss: 0.999966] [G loss: 1.000061]\n",
      "612 [D loss: 0.999971] [G loss: 1.000071]\n",
      "613 [D loss: 0.999975] [G loss: 1.000049]\n",
      "614 [D loss: 0.999972] [G loss: 1.000067]\n",
      "615 [D loss: 0.999977] [G loss: 1.000070]\n",
      "616 [D loss: 0.999968] [G loss: 1.000067]\n",
      "617 [D loss: 0.999968] [G loss: 1.000058]\n",
      "618 [D loss: 0.999962] [G loss: 1.000056]\n",
      "619 [D loss: 0.999967] [G loss: 1.000073]\n",
      "620 [D loss: 0.999979] [G loss: 1.000070]\n",
      "621 [D loss: 0.999970] [G loss: 1.000059]\n",
      "622 [D loss: 0.999965] [G loss: 1.000073]\n",
      "623 [D loss: 0.999960] [G loss: 1.000066]\n",
      "624 [D loss: 0.999971] [G loss: 1.000070]\n",
      "625 [D loss: 0.999968] [G loss: 1.000066]\n",
      "626 [D loss: 0.999976] [G loss: 1.000070]\n",
      "627 [D loss: 0.999970] [G loss: 1.000056]\n",
      "628 [D loss: 0.999971] [G loss: 1.000073]\n",
      "629 [D loss: 0.999975] [G loss: 1.000065]\n",
      "630 [D loss: 0.999968] [G loss: 1.000065]\n",
      "631 [D loss: 0.999970] [G loss: 1.000066]\n",
      "632 [D loss: 0.999973] [G loss: 1.000071]\n",
      "633 [D loss: 0.999962] [G loss: 1.000069]\n",
      "634 [D loss: 0.999972] [G loss: 1.000057]\n",
      "635 [D loss: 0.999965] [G loss: 1.000086]\n",
      "636 [D loss: 0.999970] [G loss: 1.000072]\n",
      "637 [D loss: 0.999971] [G loss: 1.000073]\n",
      "638 [D loss: 0.999966] [G loss: 1.000069]\n",
      "639 [D loss: 0.999971] [G loss: 1.000070]\n",
      "640 [D loss: 0.999966] [G loss: 1.000071]\n",
      "641 [D loss: 0.999970] [G loss: 1.000068]\n",
      "642 [D loss: 0.999970] [G loss: 1.000059]\n",
      "643 [D loss: 0.999971] [G loss: 1.000060]\n",
      "644 [D loss: 0.999970] [G loss: 1.000073]\n",
      "645 [D loss: 0.999975] [G loss: 1.000064]\n",
      "646 [D loss: 0.999965] [G loss: 1.000052]\n",
      "647 [D loss: 0.999970] [G loss: 1.000073]\n",
      "648 [D loss: 0.999966] [G loss: 1.000072]\n",
      "649 [D loss: 0.999965] [G loss: 1.000072]\n",
      "650 [D loss: 0.999971] [G loss: 1.000059]\n",
      "651 [D loss: 0.999964] [G loss: 1.000057]\n",
      "652 [D loss: 0.999975] [G loss: 1.000060]\n",
      "653 [D loss: 0.999969] [G loss: 1.000062]\n",
      "654 [D loss: 0.999967] [G loss: 1.000055]\n",
      "655 [D loss: 0.999968] [G loss: 1.000072]\n",
      "656 [D loss: 0.999967] [G loss: 1.000055]\n",
      "657 [D loss: 0.999965] [G loss: 1.000069]\n",
      "658 [D loss: 0.999971] [G loss: 1.000061]\n",
      "659 [D loss: 0.999969] [G loss: 1.000071]\n",
      "660 [D loss: 0.999964] [G loss: 1.000064]\n",
      "661 [D loss: 0.999972] [G loss: 1.000066]\n",
      "662 [D loss: 0.999975] [G loss: 1.000062]\n",
      "663 [D loss: 0.999965] [G loss: 1.000058]\n",
      "664 [D loss: 0.999959] [G loss: 1.000055]\n",
      "665 [D loss: 0.999958] [G loss: 1.000057]\n",
      "666 [D loss: 0.999967] [G loss: 1.000082]\n",
      "667 [D loss: 0.999966] [G loss: 1.000067]\n",
      "668 [D loss: 0.999970] [G loss: 1.000061]\n",
      "669 [D loss: 0.999970] [G loss: 1.000057]\n",
      "670 [D loss: 0.999964] [G loss: 1.000077]\n",
      "671 [D loss: 0.999970] [G loss: 1.000072]\n",
      "672 [D loss: 0.999972] [G loss: 1.000078]\n",
      "673 [D loss: 0.999965] [G loss: 1.000066]\n",
      "674 [D loss: 0.999977] [G loss: 1.000073]\n",
      "675 [D loss: 0.999961] [G loss: 1.000073]\n",
      "676 [D loss: 0.999961] [G loss: 1.000062]\n",
      "677 [D loss: 0.999970] [G loss: 1.000069]\n",
      "678 [D loss: 0.999963] [G loss: 1.000080]\n",
      "679 [D loss: 0.999973] [G loss: 1.000069]\n",
      "680 [D loss: 0.999964] [G loss: 1.000061]\n",
      "681 [D loss: 0.999971] [G loss: 1.000063]\n",
      "682 [D loss: 0.999965] [G loss: 1.000067]\n",
      "683 [D loss: 0.999972] [G loss: 1.000064]\n",
      "684 [D loss: 0.999972] [G loss: 1.000057]\n",
      "685 [D loss: 0.999966] [G loss: 1.000057]\n",
      "686 [D loss: 0.999971] [G loss: 1.000074]\n",
      "687 [D loss: 0.999967] [G loss: 1.000068]\n",
      "688 [D loss: 0.999971] [G loss: 1.000061]\n",
      "689 [D loss: 0.999969] [G loss: 1.000070]\n",
      "690 [D loss: 0.999963] [G loss: 1.000058]\n",
      "691 [D loss: 0.999970] [G loss: 1.000069]\n",
      "692 [D loss: 0.999966] [G loss: 1.000064]\n",
      "693 [D loss: 0.999971] [G loss: 1.000070]\n",
      "694 [D loss: 0.999971] [G loss: 1.000067]\n",
      "695 [D loss: 0.999967] [G loss: 1.000071]\n",
      "696 [D loss: 0.999969] [G loss: 1.000067]\n",
      "697 [D loss: 0.999969] [G loss: 1.000068]\n",
      "698 [D loss: 0.999969] [G loss: 1.000056]\n",
      "699 [D loss: 0.999965] [G loss: 1.000053]\n",
      "700 [D loss: 0.999971] [G loss: 1.000056]\n",
      "701 [D loss: 0.999968] [G loss: 1.000071]\n",
      "702 [D loss: 0.999966] [G loss: 1.000061]\n",
      "703 [D loss: 0.999975] [G loss: 1.000069]\n",
      "704 [D loss: 0.999975] [G loss: 1.000056]\n",
      "705 [D loss: 0.999972] [G loss: 1.000063]\n",
      "706 [D loss: 0.999973] [G loss: 1.000066]\n",
      "707 [D loss: 0.999970] [G loss: 1.000071]\n",
      "708 [D loss: 0.999969] [G loss: 1.000061]\n",
      "709 [D loss: 0.999970] [G loss: 1.000062]\n",
      "710 [D loss: 0.999976] [G loss: 1.000061]\n",
      "711 [D loss: 0.999964] [G loss: 1.000063]\n",
      "712 [D loss: 0.999969] [G loss: 1.000052]\n",
      "713 [D loss: 0.999967] [G loss: 1.000055]\n",
      "714 [D loss: 0.999967] [G loss: 1.000064]\n",
      "715 [D loss: 0.999960] [G loss: 1.000067]\n",
      "716 [D loss: 0.999970] [G loss: 1.000070]\n",
      "717 [D loss: 0.999963] [G loss: 1.000072]\n",
      "718 [D loss: 0.999969] [G loss: 1.000062]\n",
      "719 [D loss: 0.999971] [G loss: 1.000075]\n",
      "720 [D loss: 0.999967] [G loss: 1.000068]\n",
      "721 [D loss: 0.999976] [G loss: 1.000059]\n",
      "722 [D loss: 0.999970] [G loss: 1.000065]\n",
      "723 [D loss: 0.999966] [G loss: 1.000063]\n",
      "724 [D loss: 0.999968] [G loss: 1.000060]\n",
      "725 [D loss: 0.999983] [G loss: 1.000055]\n",
      "726 [D loss: 0.999973] [G loss: 1.000061]\n",
      "727 [D loss: 0.999962] [G loss: 1.000062]\n",
      "728 [D loss: 0.999967] [G loss: 1.000059]\n",
      "729 [D loss: 0.999976] [G loss: 1.000067]\n",
      "730 [D loss: 0.999971] [G loss: 1.000073]\n",
      "731 [D loss: 0.999973] [G loss: 1.000059]\n",
      "732 [D loss: 0.999972] [G loss: 1.000044]\n",
      "733 [D loss: 0.999970] [G loss: 1.000067]\n",
      "734 [D loss: 0.999972] [G loss: 1.000058]\n",
      "735 [D loss: 0.999965] [G loss: 1.000061]\n",
      "736 [D loss: 0.999969] [G loss: 1.000073]\n",
      "737 [D loss: 0.999967] [G loss: 1.000065]\n",
      "738 [D loss: 0.999971] [G loss: 1.000067]\n",
      "739 [D loss: 0.999970] [G loss: 1.000062]\n",
      "740 [D loss: 0.999968] [G loss: 1.000066]\n",
      "741 [D loss: 0.999971] [G loss: 1.000069]\n",
      "742 [D loss: 0.999974] [G loss: 1.000065]\n",
      "743 [D loss: 0.999974] [G loss: 1.000070]\n",
      "744 [D loss: 0.999972] [G loss: 1.000064]\n",
      "745 [D loss: 0.999969] [G loss: 1.000066]\n",
      "746 [D loss: 0.999965] [G loss: 1.000065]\n",
      "747 [D loss: 0.999976] [G loss: 1.000075]\n",
      "748 [D loss: 0.999974] [G loss: 1.000065]\n",
      "749 [D loss: 0.999977] [G loss: 1.000062]\n",
      "750 [D loss: 0.999968] [G loss: 1.000068]\n",
      "751 [D loss: 0.999968] [G loss: 1.000061]\n",
      "752 [D loss: 0.999973] [G loss: 1.000066]\n",
      "753 [D loss: 0.999975] [G loss: 1.000060]\n",
      "754 [D loss: 0.999966] [G loss: 1.000069]\n",
      "755 [D loss: 0.999974] [G loss: 1.000063]\n",
      "756 [D loss: 0.999967] [G loss: 1.000064]\n",
      "757 [D loss: 0.999970] [G loss: 1.000065]\n",
      "758 [D loss: 0.999971] [G loss: 1.000056]\n",
      "759 [D loss: 0.999973] [G loss: 1.000050]\n",
      "760 [D loss: 0.999969] [G loss: 1.000057]\n",
      "761 [D loss: 0.999966] [G loss: 1.000062]\n",
      "762 [D loss: 0.999965] [G loss: 1.000067]\n",
      "763 [D loss: 0.999972] [G loss: 1.000055]\n",
      "764 [D loss: 0.999961] [G loss: 1.000066]\n",
      "765 [D loss: 0.999970] [G loss: 1.000061]\n",
      "766 [D loss: 0.999966] [G loss: 1.000056]\n",
      "767 [D loss: 0.999966] [G loss: 1.000068]\n",
      "768 [D loss: 0.999964] [G loss: 1.000063]\n",
      "769 [D loss: 0.999977] [G loss: 1.000065]\n",
      "770 [D loss: 0.999971] [G loss: 1.000061]\n",
      "771 [D loss: 0.999963] [G loss: 1.000070]\n",
      "772 [D loss: 0.999972] [G loss: 1.000066]\n",
      "773 [D loss: 0.999974] [G loss: 1.000064]\n",
      "774 [D loss: 0.999970] [G loss: 1.000057]\n",
      "775 [D loss: 0.999966] [G loss: 1.000060]\n",
      "776 [D loss: 0.999971] [G loss: 1.000066]\n",
      "777 [D loss: 0.999968] [G loss: 1.000070]\n",
      "778 [D loss: 0.999972] [G loss: 1.000069]\n",
      "779 [D loss: 0.999969] [G loss: 1.000070]\n",
      "780 [D loss: 0.999968] [G loss: 1.000068]\n",
      "781 [D loss: 0.999971] [G loss: 1.000074]\n",
      "782 [D loss: 0.999975] [G loss: 1.000063]\n",
      "783 [D loss: 0.999967] [G loss: 1.000067]\n",
      "784 [D loss: 0.999969] [G loss: 1.000070]\n",
      "785 [D loss: 0.999966] [G loss: 1.000065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786 [D loss: 0.999966] [G loss: 1.000068]\n",
      "787 [D loss: 0.999963] [G loss: 1.000067]\n",
      "788 [D loss: 0.999968] [G loss: 1.000065]\n",
      "789 [D loss: 0.999972] [G loss: 1.000062]\n",
      "790 [D loss: 0.999959] [G loss: 1.000066]\n",
      "791 [D loss: 0.999966] [G loss: 1.000069]\n",
      "792 [D loss: 0.999971] [G loss: 1.000056]\n",
      "793 [D loss: 0.999963] [G loss: 1.000059]\n",
      "794 [D loss: 0.999971] [G loss: 1.000064]\n",
      "795 [D loss: 0.999969] [G loss: 1.000067]\n",
      "796 [D loss: 0.999976] [G loss: 1.000045]\n",
      "797 [D loss: 0.999970] [G loss: 1.000064]\n",
      "798 [D loss: 0.999971] [G loss: 1.000061]\n",
      "799 [D loss: 0.999970] [G loss: 1.000061]\n",
      "800 [D loss: 0.999971] [G loss: 1.000069]\n",
      "801 [D loss: 0.999974] [G loss: 1.000062]\n",
      "802 [D loss: 0.999966] [G loss: 1.000058]\n",
      "803 [D loss: 0.999973] [G loss: 1.000055]\n",
      "804 [D loss: 0.999962] [G loss: 1.000069]\n",
      "805 [D loss: 0.999976] [G loss: 1.000059]\n",
      "806 [D loss: 0.999975] [G loss: 1.000055]\n",
      "807 [D loss: 0.999970] [G loss: 1.000066]\n",
      "808 [D loss: 0.999972] [G loss: 1.000057]\n",
      "809 [D loss: 0.999975] [G loss: 1.000069]\n",
      "810 [D loss: 0.999972] [G loss: 1.000062]\n",
      "811 [D loss: 0.999970] [G loss: 1.000081]\n",
      "812 [D loss: 0.999962] [G loss: 1.000065]\n",
      "813 [D loss: 0.999973] [G loss: 1.000067]\n",
      "814 [D loss: 0.999967] [G loss: 1.000053]\n",
      "815 [D loss: 0.999969] [G loss: 1.000055]\n",
      "816 [D loss: 0.999963] [G loss: 1.000073]\n",
      "817 [D loss: 0.999970] [G loss: 1.000063]\n",
      "818 [D loss: 0.999980] [G loss: 1.000061]\n",
      "819 [D loss: 0.999955] [G loss: 1.000050]\n",
      "820 [D loss: 0.999963] [G loss: 1.000058]\n",
      "821 [D loss: 0.999965] [G loss: 1.000063]\n",
      "822 [D loss: 0.999971] [G loss: 1.000045]\n",
      "823 [D loss: 0.999972] [G loss: 1.000069]\n",
      "824 [D loss: 0.999967] [G loss: 1.000069]\n",
      "825 [D loss: 0.999967] [G loss: 1.000072]\n",
      "826 [D loss: 0.999962] [G loss: 1.000059]\n",
      "827 [D loss: 0.999974] [G loss: 1.000052]\n",
      "828 [D loss: 0.999964] [G loss: 1.000072]\n",
      "829 [D loss: 0.999969] [G loss: 1.000076]\n",
      "830 [D loss: 0.999966] [G loss: 1.000057]\n",
      "831 [D loss: 0.999970] [G loss: 1.000057]\n",
      "832 [D loss: 0.999967] [G loss: 1.000057]\n",
      "833 [D loss: 0.999971] [G loss: 1.000068]\n",
      "834 [D loss: 0.999968] [G loss: 1.000071]\n",
      "835 [D loss: 0.999967] [G loss: 1.000060]\n",
      "836 [D loss: 0.999970] [G loss: 1.000058]\n",
      "837 [D loss: 0.999974] [G loss: 1.000063]\n",
      "838 [D loss: 0.999974] [G loss: 1.000063]\n",
      "839 [D loss: 0.999973] [G loss: 1.000067]\n",
      "840 [D loss: 0.999970] [G loss: 1.000062]\n",
      "841 [D loss: 0.999969] [G loss: 1.000067]\n",
      "842 [D loss: 0.999965] [G loss: 1.000060]\n",
      "843 [D loss: 0.999969] [G loss: 1.000061]\n",
      "844 [D loss: 0.999966] [G loss: 1.000063]\n",
      "845 [D loss: 0.999971] [G loss: 1.000059]\n",
      "846 [D loss: 0.999967] [G loss: 1.000064]\n",
      "847 [D loss: 0.999965] [G loss: 1.000067]\n",
      "848 [D loss: 0.999970] [G loss: 1.000057]\n",
      "849 [D loss: 0.999968] [G loss: 1.000051]\n",
      "850 [D loss: 0.999968] [G loss: 1.000070]\n",
      "851 [D loss: 0.999968] [G loss: 1.000071]\n",
      "852 [D loss: 0.999968] [G loss: 1.000066]\n",
      "853 [D loss: 0.999974] [G loss: 1.000069]\n",
      "854 [D loss: 0.999973] [G loss: 1.000071]\n",
      "855 [D loss: 0.999975] [G loss: 1.000056]\n",
      "856 [D loss: 0.999964] [G loss: 1.000067]\n",
      "857 [D loss: 0.999965] [G loss: 1.000069]\n",
      "858 [D loss: 0.999964] [G loss: 1.000063]\n",
      "859 [D loss: 0.999970] [G loss: 1.000068]\n",
      "860 [D loss: 0.999966] [G loss: 1.000072]\n",
      "861 [D loss: 0.999971] [G loss: 1.000068]\n",
      "862 [D loss: 0.999970] [G loss: 1.000056]\n",
      "863 [D loss: 0.999966] [G loss: 1.000063]\n",
      "864 [D loss: 0.999964] [G loss: 1.000059]\n",
      "865 [D loss: 0.999971] [G loss: 1.000069]\n",
      "866 [D loss: 0.999972] [G loss: 1.000059]\n",
      "867 [D loss: 0.999974] [G loss: 1.000067]\n",
      "868 [D loss: 0.999975] [G loss: 1.000057]\n",
      "869 [D loss: 0.999970] [G loss: 1.000070]\n",
      "870 [D loss: 0.999964] [G loss: 1.000066]\n",
      "871 [D loss: 0.999971] [G loss: 1.000066]\n",
      "872 [D loss: 0.999969] [G loss: 1.000069]\n",
      "873 [D loss: 0.999972] [G loss: 1.000060]\n",
      "874 [D loss: 0.999972] [G loss: 1.000069]\n",
      "875 [D loss: 0.999970] [G loss: 1.000072]\n",
      "876 [D loss: 0.999974] [G loss: 1.000067]\n",
      "877 [D loss: 0.999966] [G loss: 1.000067]\n",
      "878 [D loss: 0.999964] [G loss: 1.000073]\n",
      "879 [D loss: 0.999972] [G loss: 1.000062]\n",
      "880 [D loss: 0.999973] [G loss: 1.000068]\n",
      "881 [D loss: 0.999976] [G loss: 1.000062]\n",
      "882 [D loss: 0.999967] [G loss: 1.000058]\n",
      "883 [D loss: 0.999967] [G loss: 1.000059]\n",
      "884 [D loss: 0.999961] [G loss: 1.000059]\n",
      "885 [D loss: 0.999968] [G loss: 1.000059]\n",
      "886 [D loss: 0.999968] [G loss: 1.000054]\n",
      "887 [D loss: 0.999967] [G loss: 1.000071]\n",
      "888 [D loss: 0.999967] [G loss: 1.000056]\n",
      "889 [D loss: 0.999969] [G loss: 1.000075]\n",
      "890 [D loss: 0.999972] [G loss: 1.000059]\n",
      "891 [D loss: 0.999965] [G loss: 1.000070]\n",
      "892 [D loss: 0.999967] [G loss: 1.000067]\n",
      "893 [D loss: 0.999971] [G loss: 1.000053]\n",
      "894 [D loss: 0.999967] [G loss: 1.000061]\n",
      "895 [D loss: 0.999964] [G loss: 1.000071]\n",
      "896 [D loss: 0.999970] [G loss: 1.000064]\n",
      "897 [D loss: 0.999970] [G loss: 1.000060]\n",
      "898 [D loss: 0.999974] [G loss: 1.000050]\n",
      "899 [D loss: 0.999969] [G loss: 1.000070]\n",
      "900 [D loss: 0.999971] [G loss: 1.000064]\n",
      "901 [D loss: 0.999971] [G loss: 1.000054]\n",
      "902 [D loss: 0.999960] [G loss: 1.000062]\n",
      "903 [D loss: 0.999976] [G loss: 1.000072]\n",
      "904 [D loss: 0.999971] [G loss: 1.000059]\n",
      "905 [D loss: 0.999958] [G loss: 1.000059]\n",
      "906 [D loss: 0.999969] [G loss: 1.000058]\n",
      "907 [D loss: 0.999971] [G loss: 1.000062]\n",
      "908 [D loss: 0.999966] [G loss: 1.000062]\n",
      "909 [D loss: 0.999977] [G loss: 1.000065]\n",
      "910 [D loss: 0.999964] [G loss: 1.000058]\n",
      "911 [D loss: 0.999973] [G loss: 1.000064]\n",
      "912 [D loss: 0.999968] [G loss: 1.000057]\n",
      "913 [D loss: 0.999967] [G loss: 1.000052]\n",
      "914 [D loss: 0.999968] [G loss: 1.000065]\n",
      "915 [D loss: 0.999972] [G loss: 1.000066]\n",
      "916 [D loss: 0.999967] [G loss: 1.000053]\n",
      "917 [D loss: 0.999975] [G loss: 1.000068]\n",
      "918 [D loss: 0.999974] [G loss: 1.000056]\n",
      "919 [D loss: 0.999971] [G loss: 1.000074]\n",
      "920 [D loss: 0.999974] [G loss: 1.000058]\n",
      "921 [D loss: 0.999965] [G loss: 1.000068]\n",
      "922 [D loss: 0.999965] [G loss: 1.000049]\n",
      "923 [D loss: 0.999967] [G loss: 1.000074]\n",
      "924 [D loss: 0.999972] [G loss: 1.000067]\n",
      "925 [D loss: 0.999974] [G loss: 1.000072]\n",
      "926 [D loss: 0.999972] [G loss: 1.000053]\n",
      "927 [D loss: 0.999975] [G loss: 1.000066]\n",
      "928 [D loss: 0.999964] [G loss: 1.000060]\n",
      "929 [D loss: 0.999976] [G loss: 1.000068]\n",
      "930 [D loss: 0.999965] [G loss: 1.000071]\n",
      "931 [D loss: 0.999980] [G loss: 1.000062]\n",
      "932 [D loss: 0.999967] [G loss: 1.000052]\n",
      "933 [D loss: 0.999967] [G loss: 1.000077]\n",
      "934 [D loss: 0.999968] [G loss: 1.000074]\n",
      "935 [D loss: 0.999969] [G loss: 1.000050]\n",
      "936 [D loss: 0.999975] [G loss: 1.000058]\n",
      "937 [D loss: 0.999965] [G loss: 1.000073]\n",
      "938 [D loss: 0.999971] [G loss: 1.000061]\n",
      "939 [D loss: 0.999966] [G loss: 1.000073]\n",
      "940 [D loss: 0.999968] [G loss: 1.000059]\n",
      "941 [D loss: 0.999969] [G loss: 1.000068]\n",
      "942 [D loss: 0.999967] [G loss: 1.000063]\n",
      "943 [D loss: 0.999963] [G loss: 1.000069]\n",
      "944 [D loss: 0.999973] [G loss: 1.000065]\n",
      "945 [D loss: 0.999970] [G loss: 1.000058]\n",
      "946 [D loss: 0.999962] [G loss: 1.000056]\n",
      "947 [D loss: 0.999974] [G loss: 1.000059]\n",
      "948 [D loss: 0.999969] [G loss: 1.000065]\n",
      "949 [D loss: 0.999975] [G loss: 1.000070]\n",
      "950 [D loss: 0.999967] [G loss: 1.000055]\n",
      "951 [D loss: 0.999973] [G loss: 1.000065]\n",
      "952 [D loss: 0.999965] [G loss: 1.000060]\n",
      "953 [D loss: 0.999972] [G loss: 1.000060]\n",
      "954 [D loss: 0.999970] [G loss: 1.000059]\n",
      "955 [D loss: 0.999969] [G loss: 1.000075]\n",
      "956 [D loss: 0.999966] [G loss: 1.000055]\n",
      "957 [D loss: 0.999977] [G loss: 1.000057]\n",
      "958 [D loss: 0.999975] [G loss: 1.000064]\n",
      "959 [D loss: 0.999972] [G loss: 1.000069]\n",
      "960 [D loss: 0.999976] [G loss: 1.000069]\n",
      "961 [D loss: 0.999977] [G loss: 1.000063]\n",
      "962 [D loss: 0.999967] [G loss: 1.000060]\n",
      "963 [D loss: 0.999971] [G loss: 1.000061]\n",
      "964 [D loss: 0.999967] [G loss: 1.000063]\n",
      "965 [D loss: 0.999973] [G loss: 1.000055]\n",
      "966 [D loss: 0.999968] [G loss: 1.000057]\n",
      "967 [D loss: 0.999973] [G loss: 1.000072]\n",
      "968 [D loss: 0.999976] [G loss: 1.000064]\n",
      "969 [D loss: 0.999972] [G loss: 1.000065]\n",
      "970 [D loss: 0.999973] [G loss: 1.000064]\n",
      "971 [D loss: 0.999971] [G loss: 1.000070]\n",
      "972 [D loss: 0.999967] [G loss: 1.000066]\n",
      "973 [D loss: 0.999969] [G loss: 1.000066]\n",
      "974 [D loss: 0.999967] [G loss: 1.000066]\n",
      "975 [D loss: 0.999963] [G loss: 1.000062]\n",
      "976 [D loss: 0.999972] [G loss: 1.000064]\n",
      "977 [D loss: 0.999972] [G loss: 1.000069]\n",
      "978 [D loss: 0.999970] [G loss: 1.000074]\n",
      "979 [D loss: 0.999975] [G loss: 1.000062]\n",
      "980 [D loss: 0.999974] [G loss: 1.000068]\n",
      "981 [D loss: 0.999975] [G loss: 1.000060]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "982 [D loss: 0.999967] [G loss: 1.000062]\n",
      "983 [D loss: 0.999968] [G loss: 1.000052]\n",
      "984 [D loss: 0.999966] [G loss: 1.000061]\n",
      "985 [D loss: 0.999968] [G loss: 1.000070]\n",
      "986 [D loss: 0.999960] [G loss: 1.000075]\n",
      "987 [D loss: 0.999970] [G loss: 1.000066]\n",
      "988 [D loss: 0.999975] [G loss: 1.000057]\n",
      "989 [D loss: 0.999969] [G loss: 1.000069]\n",
      "990 [D loss: 0.999971] [G loss: 1.000074]\n",
      "991 [D loss: 0.999974] [G loss: 1.000056]\n",
      "992 [D loss: 0.999971] [G loss: 1.000059]\n",
      "993 [D loss: 0.999965] [G loss: 1.000065]\n",
      "994 [D loss: 0.999968] [G loss: 1.000056]\n",
      "995 [D loss: 0.999968] [G loss: 1.000064]\n",
      "996 [D loss: 0.999968] [G loss: 1.000076]\n",
      "997 [D loss: 0.999975] [G loss: 1.000068]\n",
      "998 [D loss: 0.999971] [G loss: 1.000060]\n",
      "999 [D loss: 0.999967] [G loss: 1.000068]\n"
     ]
    }
   ],
   "source": [
    "wgan.train(epochs=1000, batch_size=32, sample_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLoss(train, test):\n",
    "    iterations = list(range(len(train)))\n",
    "    plt.plot(iterations, train, label=\"Discriminator\")\n",
    "    plt.plot(iterations, test, label=\"Generator\")       \n",
    "\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True, linestyle='dotted')\n",
    "    plt.legend() #loc=\"upper left\"\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dloss = [x[0] for x in wgan.d_loss]\n",
    "gloss = [x[0] for x in wgan.g_loss]\n",
    "plotLoss(dloss, gloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def photogallery(photos):\n",
    "    fig = plt.figure(figsize=(4, 12))\n",
    "    for i, entry in enumerate(photos):\n",
    "        ax1 = fig.add_subplot(10,2,i*2+1)\n",
    "        ax1.imshow(entry[0])\n",
    "        ax2 = fig.add_subplot(10,2,i*2+2)\n",
    "        ax2.imshow(entry[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "digits = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "photos = []\n",
    "\n",
    "print(\"original, vae\")\n",
    "for digit in digits:\n",
    "    index0 = np.where(y_test==digit)[0]\n",
    "    pick = index0[np.random.choice(index0.shape[0])]\n",
    "    #digit_indexes.append(pick)\n",
    "    #print(pick)\n",
    "    #print(im_shape)\n",
    "    original = x_test[pick].reshape(32, 32, 3)\n",
    "    encoded = x_test_encoded[pick].reshape(32, 32,3)\n",
    "    photos.append([original, encoded])\n",
    "    \n",
    "photogallery(photos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGeneratedImages(epoch, examples=40, dim=(10, 4), figsize=(10, 4)):\n",
    "    noise = np.random.normal(0, 1, size=[examples, randomDim])\n",
    "    generatedImages = generator.predict(noise)\n",
    "    generatedImages = generatedImages.reshape(examples, 28, 28)\n",
    "\n",
    "    plt.figure()\n",
    "    for i in range(generatedImages.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generatedImages[i], interpolation='nearest', cmap=plt.cm.gray)\n",
    "        plt.axis('off')\n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "    #plt.savefig('images/gan_generated_image_epoch_%d.png' % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 40 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotGeneratedImages(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
